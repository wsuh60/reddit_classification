{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "focus": false,
    "id": "69b9a648-bcc7-490d-9f9b-ea244d156bd6"
   },
   "source": [
    "\n",
    "\n",
    "# ![](https://ga-dash.s3.amazonaws.com/production/assets/logo-9f88ae6c9c3871690e33280fcf557f33.png)  Using Reddit's API for Predicting Comments<br>\n",
    "### Author: Will J. Suh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-10-23T19:28:02.619411Z",
     "start_time": "2017-10-23T19:28:02.600856Z"
    }
   },
   "source": [
    "In this project, we will practice two major skills. Collecting data via an API request and then building a binary predictor.\n",
    "\n",
    "As we discussed in week 2, and earlier today, there are two components to starting a data science problem: the problem statement, and acquiring the data.\n",
    "\n",
    "For this article, your problem statement will be: **_What characteristics of a post on Reddit contribute most to what subreddit it belongs to?_**\n",
    "\n",
    "Your method for acquiring the data will be scraping threads from at least two subreddits. \n",
    "\n",
    "Once you've got the data, you will build a classification model that, using Natural Language Processing and any other relevant features, predicts which subreddit a given post belongs to."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "focus": false,
    "id": "a948d79c-5527-4c0d-ab23-f5d43ce72056"
   },
   "source": [
    "### Scraping Thread Info from Reddit.com"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Set up a request (using requests) to the URL below. \n",
    "\n",
    "*NOTE*: Reddit will throw a [429 error](https://httpstatuses.com/429) when using the following code:\n",
    "```python\n",
    "res = requests.get(URL)\n",
    "```\n",
    "\n",
    "This is because Reddit has throttled python's default user agent. You'll need to set a custom `User-agent` to get your request to work.\n",
    "```python\n",
    "res = requests.get(URL, headers={'User-agent': 'YOUR NAME Bot 0.1'})\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/queenbee/anaconda3/lib/python3.6/site-packages/sklearn/cross_validation.py:41: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import json\n",
    "from bs4 import BeautifulSoup    \n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import requests\n",
    "import re\n",
    "from sklearn.cross_validation import train_test_split, cross_val_score\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "URL = 'https://www.reddit.com/r/todayilearned.json'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Use `res.json()` to convert the response into a dictionary format and set this to a variable. \n",
    "\n",
    "```python\n",
    "data = res.json()\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = requests.get(URL, headers = {'User-agent' : 'Will Bot 0.1'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = results.json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25\n"
     ]
    }
   ],
   "source": [
    "print(len(data['data']['children']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['data'].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'t3_9eecnq'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['data']['after']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#after = \n",
    "new_url = 'http://www.reddit.com/r/todayilearned.json?after=t3_9dkoki'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results = requests.get(new_url, headers = {'User-agent':'Will Bot 0.1'})\n",
    "new_data = results.json()\n",
    "new_data['data']['children']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "soup = BeautifulSoup(results.text, 'lxml')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(new_data['data']['children'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Getting more results\n",
    "\n",
    "By default, Reddit will give you the top 25 posts:\n",
    "\n",
    "```python\n",
    "print(len(data['data']['children']))\n",
    "```\n",
    "\n",
    "If you want more, you'll need to do two things:\n",
    "1. Get the name of the last post: `data['data']['after']`\n",
    "2. Use that name to hit the following url: `http://www.reddit.com/r/boardgames.json?after=THE_AFTER_FROM_STEP_1`\n",
    "3. Create a loop to repeat steps 1 and 2 until you have a sufficient number of posts. \n",
    "\n",
    "*NOTE*: Reddit will limit the number of requests per second you're allowed to make. When you create your loop, be sure to add the following after each iteration.\n",
    "\n",
    "```python\n",
    "time.sleep(3) # sleeps 3 seconds before continuing```\n",
    "\n",
    "This will throttle your loop and keep you within Reddit's guidelines. You'll need to import the `time` library for this to work!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## First subreddit: /r/todayilearned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The current after:  t3_9eecnq\n",
      "The current after:  t3_9eev2n\n",
      "The current after:  t3_9efh4r\n",
      "The current after:  t3_9eajh3\n",
      "The current after:  t3_9e871c\n",
      "The current after:  t3_9e2xb5\n",
      "The current after:  t3_9e2ih3\n",
      "The current after:  t3_9dyqwo\n",
      "The current after:  t3_9dxamf\n",
      "The current after:  t3_9dgksr\n",
      "The current after:  t3_9dqo5o\n",
      "The current after:  t3_9dmcb4\n",
      "The current after:  t3_9dqzu2\n",
      "The current after:  t3_9do3aa\n",
      "The current after:  t3_9dma8z\n",
      "The current after:  t3_9df5t1\n",
      "The current after:  t3_9df4ig\n",
      "The current after:  t3_9dlqcv\n",
      "The current after:  t3_9d9dbe\n",
      "The current after:  t3_9d7veb\n",
      "The current after:  t3_9d4hnx\n",
      "The current after:  t3_9cpvk2\n",
      "The current after:  t3_9czmxp\n",
      "The current after:  t3_9cwjda\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "must be str, not NoneType",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-26-d8200c4fa491>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     21\u001b[0m     \u001b[0;31m# reassign the after to the current 'after', and then update the url to hit\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m     \u001b[0mafter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'data'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'after'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m     \u001b[0murl\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'http://www.reddit.com/r/todayilearned.json?after='\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mafter\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: must be str, not NoneType"
     ]
    }
   ],
   "source": [
    "url = 'http://www.reddit.com/r/todayilearned.json'\n",
    "all_posts =[]\n",
    "for _ in range(40): \n",
    "    # construct a list of 1000\n",
    "    \n",
    "    # Get the posts by hitting the url, put it in json and store it\n",
    "    res = requests.get(url, headers={'User-agent': 'will bot'})\n",
    "    data = res.json()\n",
    "    \n",
    "    # save only the posts out of the json into the list_of_posts, then\n",
    "    # add all the posts to the all_posts list\n",
    "    list_of_posts = data['data']['children']\n",
    "    \n",
    "    for post in list_of_posts:\n",
    "        current_post = []\n",
    "        current_post.append(post['data']['selftext'])\n",
    "        current_post.append(post['data']['title'])\n",
    "        current_post.append(post['data']['subreddit_name_prefixed'])\n",
    "        all_posts.append(current_post)\n",
    "    \n",
    "    # reassign the after to the current 'after', and then update the url to hit\n",
    "    after = data['data']['after']\n",
    "    url = 'http://www.reddit.com/r/todayilearned.json?after=' + after\n",
    "    \n",
    "    \n",
    "    # go to sleep for 3 seconds so you do not overwhelm reddit and get kicked out\n",
    "    print('The current after: ', after)\n",
    "    time.sleep(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save this to a dataframe first and then to CSV\n",
    "\n",
    "first_subreddit = pd.DataFrame(all_posts, columns = ['post', 'title', 'true_y'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>post</th>\n",
       "      <th>title</th>\n",
       "      <th>true_y</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td></td>\n",
       "      <td>TIL the monarch butterfly’s life span is 2 to ...</td>\n",
       "      <td>r/todayilearned</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td></td>\n",
       "      <td>TIL That Bob Hawke is the only Australian prim...</td>\n",
       "      <td>r/todayilearned</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td></td>\n",
       "      <td>TIL A man called Miyamoto Musashi is considere...</td>\n",
       "      <td>r/todayilearned</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td></td>\n",
       "      <td>TIL the Barbie Liberation Organization swapped...</td>\n",
       "      <td>r/todayilearned</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td></td>\n",
       "      <td>TIL that although bamboo are commonly refered ...</td>\n",
       "      <td>r/todayilearned</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  post                                              title           true_y\n",
       "0       TIL the monarch butterfly’s life span is 2 to ...  r/todayilearned\n",
       "1       TIL That Bob Hawke is the only Australian prim...  r/todayilearned\n",
       "2       TIL A man called Miyamoto Musashi is considere...  r/todayilearned\n",
       "3       TIL the Barbie Liberation Organization swapped...  r/todayilearned\n",
       "4       TIL that although bamboo are commonly refered ...  r/todayilearned"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "first_subreddit.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "focus": false,
    "id": "43e71edd-210e-42b1-9336-70a931f048af"
   },
   "source": [
    "### Save your results as a CSV\n",
    "You may do this regularly while scraping data as well, so that if your scraper stops of your computer crashes, you don't lose all your data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(first_subreddit).to_csv('./TIL') #renamed to first_subreddit in jupyter notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Second subreddit: /r/askreddit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11\n"
     ]
    }
   ],
   "source": [
    "url2 = 'https://www.reddit.com/r/askreddit.json'\n",
    "results2 = requests.get(url2, headers = {'User-agent' : 'Will Bot 0.2'})\n",
    "data2 = results2.json()\n",
    "print(len(data['data']['children']))\n",
    "\n",
    "soup = BeautifulSoup(results2.text, 'lxml')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The current after:  t3_9e3rrq\n",
      "The current after:  t3_9ec5nk\n",
      "The current after:  t3_9eefpl\n",
      "The current after:  t3_9e5rrb\n",
      "The current after:  t3_9ednp8\n",
      "The current after:  t3_9efeot\n",
      "The current after:  t3_9ef7ak\n",
      "The current after:  t3_9eezj3\n",
      "The current after:  t3_9efjg0\n",
      "The current after:  t3_9efexp\n",
      "The current after:  t3_9ef97c\n",
      "The current after:  t3_9ef4n8\n",
      "The current after:  t3_9edmq7\n",
      "The current after:  t3_9ed3ey\n",
      "The current after:  t3_9eenw8\n",
      "The current after:  t3_9efmyu\n",
      "The current after:  t3_9efjij\n",
      "The current after:  t3_9efgdm\n",
      "The current after:  t3_9edjyo\n",
      "The current after:  t3_9ef917\n",
      "The current after:  t3_9ef5p0\n",
      "The current after:  t3_9ef2y6\n",
      "The current after:  t3_9eax3g\n",
      "The current after:  t3_9eeyb8\n",
      "The current after:  t3_9eeu8r\n",
      "The current after:  t3_9edowb\n",
      "The current after:  t3_9eawcv\n",
      "The current after:  t3_9ediz0\n",
      "The current after:  t3_9eeg0a\n",
      "The current after:  t3_9eecn6\n",
      "The current after:  t3_9ee981\n",
      "The current after:  t3_9ee5a3\n",
      "The current after:  t3_9ed1c5\n",
      "The current after:  t3_9efpmu\n",
      "The current after:  t3_9efned\n",
      "The current after:  t3_9e9jeh\n",
      "The current after:  t3_9efjsh\n",
      "The current after:  t3_9edolp\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "must be str, not NoneType",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-62-becf116591b7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     21\u001b[0m     \u001b[0;31m# reassign the after to the current 'after', and then update the url to hit\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m     \u001b[0mafter2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata2\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'data'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'after'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m     \u001b[0murl2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'https://www.reddit.com/r/askreddit.json?after='\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mafter2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m     \u001b[0;31m# go to sleep for 3 seconds so you do not overwhelm reddit and get kicked out\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: must be str, not NoneType"
     ]
    }
   ],
   "source": [
    "url2 = 'https://www.reddit.com/r/askreddit.json'\n",
    "all_posts =[]\n",
    "for _ in range(40): \n",
    "    # construct a list of 1000\n",
    "    \n",
    "    # Get the posts by hitting the url, put it in json and store it\n",
    "    res = requests.get(url2, headers={'User-agent': 'will bot 0.2'})\n",
    "    data2 = res.json()\n",
    "    \n",
    "    # save only the posts out of the json into the list_of_posts, then\n",
    "    # add all the posts to the all_posts list\n",
    "    list_of_posts = data2['data']['children']\n",
    "    \n",
    "    for post in list_of_posts:\n",
    "        current_post = []\n",
    "        current_post.append(post['data']['selftext'])\n",
    "        current_post.append(post['data']['title'])\n",
    "        current_post.append(post['data']['subreddit_name_prefixed'])\n",
    "        all_posts.append(current_post)\n",
    "    \n",
    "    # reassign the after to the current 'after', and then update the url to hit\n",
    "    after2 = data2['data']['after']\n",
    "    url2 = 'https://www.reddit.com/r/askreddit.json?after=' + after2\n",
    "    \n",
    "    # go to sleep for 3 seconds so you do not overwhelm reddit and get kicked out\n",
    "    print('The current after: ', after2)\n",
    "    time.sleep(5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "956"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(all_posts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "second_subreddit = pd.DataFrame(all_posts, columns = ['post', 'title', 'true_y'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>post</th>\n",
       "      <th>title</th>\n",
       "      <th>true_y</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>We'll use this as a celebration post. Subreddi...</td>\n",
       "      <td>AskReddit has reached 20 million subscribers!</td>\n",
       "      <td>r/AskReddit</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td></td>\n",
       "      <td>What's a \"fact\" you thought was true for years...</td>\n",
       "      <td>r/AskReddit</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td></td>\n",
       "      <td>NSFW what's the most awkward boner you've ever...</td>\n",
       "      <td>r/AskReddit</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td></td>\n",
       "      <td>Reddit, what's a good icebreaker (for parties,...</td>\n",
       "      <td>r/AskReddit</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td></td>\n",
       "      <td>What's something people think makes them uniqu...</td>\n",
       "      <td>r/AskReddit</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                post  \\\n",
       "0  We'll use this as a celebration post. Subreddi...   \n",
       "1                                                      \n",
       "2                                                      \n",
       "3                                                      \n",
       "4                                                      \n",
       "\n",
       "                                               title       true_y  \n",
       "0      AskReddit has reached 20 million subscribers!  r/AskReddit  \n",
       "1  What's a \"fact\" you thought was true for years...  r/AskReddit  \n",
       "2  NSFW what's the most awkward boner you've ever...  r/AskReddit  \n",
       "3  Reddit, what's a good icebreaker (for parties,...  r/AskReddit  \n",
       "4  What's something people think makes them uniqu...  r/AskReddit  "
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "second_subreddit.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(second_subreddit).to_csv('./askreddit')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load CSV and Clean Dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "til = pd.read_csv('./TIL')\n",
    "ask = pd.read_csv('./askreddit')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "del til['Unnamed: 0']\n",
    "del ask['Unnamed: 0']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>post</th>\n",
       "      <th>title</th>\n",
       "      <th>true_y</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>We'll use this as a celebration post. Subreddi...</td>\n",
       "      <td>AskReddit has reached 20 million subscribers!</td>\n",
       "      <td>r/AskReddit</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>NaN</td>\n",
       "      <td>What's a \"fact\" you thought was true for years...</td>\n",
       "      <td>r/AskReddit</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NSFW what's the most awkward boner you've ever...</td>\n",
       "      <td>r/AskReddit</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>NaN</td>\n",
       "      <td>Reddit, what's a good icebreaker (for parties,...</td>\n",
       "      <td>r/AskReddit</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>NaN</td>\n",
       "      <td>What's something people think makes them uniqu...</td>\n",
       "      <td>r/AskReddit</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                post  \\\n",
       "0  We'll use this as a celebration post. Subreddi...   \n",
       "1                                                NaN   \n",
       "2                                                NaN   \n",
       "3                                                NaN   \n",
       "4                                                NaN   \n",
       "\n",
       "                                               title       true_y  \n",
       "0      AskReddit has reached 20 million subscribers!  r/AskReddit  \n",
       "1  What's a \"fact\" you thought was true for years...  r/AskReddit  \n",
       "2  NSFW what's the most awkward boner you've ever...  r/AskReddit  \n",
       "3  Reddit, what's a good icebreaker (for parties,...  r/AskReddit  \n",
       "4  What's something people think makes them uniqu...  r/AskReddit  "
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ask.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(956, 3)"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ask.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(611, 3)"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "til.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>post</th>\n",
       "      <th>title</th>\n",
       "      <th>true_y</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>NaN</td>\n",
       "      <td>TIL the monarch butterfly’s life span is 2 to ...</td>\n",
       "      <td>r/todayilearned</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>NaN</td>\n",
       "      <td>TIL That Bob Hawke is the only Australian prim...</td>\n",
       "      <td>r/todayilearned</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>NaN</td>\n",
       "      <td>TIL A man called Miyamoto Musashi is considere...</td>\n",
       "      <td>r/todayilearned</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>NaN</td>\n",
       "      <td>TIL the Barbie Liberation Organization swapped...</td>\n",
       "      <td>r/todayilearned</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>NaN</td>\n",
       "      <td>TIL that although bamboo are commonly refered ...</td>\n",
       "      <td>r/todayilearned</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   post                                              title           true_y\n",
       "0   NaN  TIL the monarch butterfly’s life span is 2 to ...  r/todayilearned\n",
       "1   NaN  TIL That Bob Hawke is the only Australian prim...  r/todayilearned\n",
       "2   NaN  TIL A man called Miyamoto Musashi is considere...  r/todayilearned\n",
       "3   NaN  TIL the Barbie Liberation Organization swapped...  r/todayilearned\n",
       "4   NaN  TIL that although bamboo are commonly refered ...  r/todayilearned"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "til.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are three observations in 'AskReddit' where there is a post. \n",
    "\n",
    "There are no observations in 'Today I Learned' where there is a post.\n",
    "\n",
    "I will create a feature that combines post and title instead of deleting the post feature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "post      611\n",
       "title       0\n",
       "true_y      0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "til.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "post      954\n",
       "title       0\n",
       "true_y      0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ask.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "ask['post'] = ask['post'].fillna('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "til['post'] = til['post'].fillna('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Post + Title Feat. Eng.\n",
    "\n",
    "ask['interact'] = ask['post'] + ask['title']\n",
    "til['interact'] = til['post'] + til['title']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>post</th>\n",
       "      <th>title</th>\n",
       "      <th>true_y</th>\n",
       "      <th>interact</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>We'll use this as a celebration post. Subreddi...</td>\n",
       "      <td>AskReddit has reached 20 million subscribers!</td>\n",
       "      <td>r/AskReddit</td>\n",
       "      <td>We'll use this as a celebration post. Subreddi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td></td>\n",
       "      <td>What's a \"fact\" you thought was true for years...</td>\n",
       "      <td>r/AskReddit</td>\n",
       "      <td>What's a \"fact\" you thought was true for years...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td></td>\n",
       "      <td>NSFW what's the most awkward boner you've ever...</td>\n",
       "      <td>r/AskReddit</td>\n",
       "      <td>NSFW what's the most awkward boner you've ever...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td></td>\n",
       "      <td>Reddit, what's a good icebreaker (for parties,...</td>\n",
       "      <td>r/AskReddit</td>\n",
       "      <td>Reddit, what's a good icebreaker (for parties,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td></td>\n",
       "      <td>What's something people think makes them uniqu...</td>\n",
       "      <td>r/AskReddit</td>\n",
       "      <td>What's something people think makes them uniqu...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                post  \\\n",
       "0  We'll use this as a celebration post. Subreddi...   \n",
       "1                                                      \n",
       "2                                                      \n",
       "3                                                      \n",
       "4                                                      \n",
       "\n",
       "                                               title       true_y  \\\n",
       "0      AskReddit has reached 20 million subscribers!  r/AskReddit   \n",
       "1  What's a \"fact\" you thought was true for years...  r/AskReddit   \n",
       "2  NSFW what's the most awkward boner you've ever...  r/AskReddit   \n",
       "3  Reddit, what's a good icebreaker (for parties,...  r/AskReddit   \n",
       "4  What's something people think makes them uniqu...  r/AskReddit   \n",
       "\n",
       "                                            interact  \n",
       "0  We'll use this as a celebration post. Subreddi...  \n",
       "1  What's a \"fact\" you thought was true for years...  \n",
       "2  NSFW what's the most awkward boner you've ever...  \n",
       "3  Reddit, what's a good icebreaker (for parties,...  \n",
       "4  What's something people think makes them uniqu...  "
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ask.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Join TIL and AskReddit into a dataframe called concat_df\n",
    "\n",
    "concat_df = pd.concat([til, ask], axis = 0, ignore_index= True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clean stop words:\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "stop = stopwords.words('english')\n",
    "\n",
    "concat_df['interact'] = concat_df['interact'].apply(lambda x: ' '.join([word for word in x.split(' ') if word not in (stop)]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove \"TIL\" from concat_df['interact'] because it would be too easy.\n",
    "# Remove \"?\" from concat_df['interact'] because almost all of AskReddit ends in a question mark.\n",
    "# Remove other punctuation marks and lower case everything.\n",
    "\n",
    "artifact = ['\\n\\n', '\\n', '\\t', '#', r'r/', 'https://', 'www.', 'http://', 'reddit.com', 'wiki',\n",
    "            '.', ',', '-',';','\"', ':', \"[\", \"[\", \"(\", \")\", '[/', '/', '?', \"'\", '*', '$', '&', \n",
    "            'TIL', 'the', 'reddit', 'it', 'be']\n",
    "for i in artifact:\n",
    "    concat_df['interact'] = concat_df['interact'].map(lambda x: x.replace(i, ''))\n",
    "\n",
    "concat_df['interact'] = concat_df['interact'].str.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "# one hot encode true_y. 0 == 'AskReddit' / 1 == 'TIL'\n",
    "\n",
    "one_hot = pd.get_dummies(concat_df['true_y'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>post</th>\n",
       "      <th>title</th>\n",
       "      <th>true_y</th>\n",
       "      <th>interact</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1562</th>\n",
       "      <td></td>\n",
       "      <td>People who have been to therapy have you ever ...</td>\n",
       "      <td>r/AskReddit</td>\n",
       "      <td>people rapy ever thought dating rapist getting...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1563</th>\n",
       "      <td></td>\n",
       "      <td>Feeling worthless and empty at times. Not sad,...</td>\n",
       "      <td>r/AskReddit</td>\n",
       "      <td>feeling worthless empty times not sad hollow w...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1564</th>\n",
       "      <td></td>\n",
       "      <td>What is the genre of music in the mr green com...</td>\n",
       "      <td>r/AskReddit</td>\n",
       "      <td>what genre music mr green commercial</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1565</th>\n",
       "      <td></td>\n",
       "      <td>What is your \"Better them than me\" moment?</td>\n",
       "      <td>r/AskReddit</td>\n",
       "      <td>what better me moment</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1566</th>\n",
       "      <td></td>\n",
       "      <td>What are some of the creepiest things you have...</td>\n",
       "      <td>r/AskReddit</td>\n",
       "      <td>what creepiest things found internet</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     post                                              title       true_y  \\\n",
       "1562       People who have been to therapy have you ever ...  r/AskReddit   \n",
       "1563       Feeling worthless and empty at times. Not sad,...  r/AskReddit   \n",
       "1564       What is the genre of music in the mr green com...  r/AskReddit   \n",
       "1565              What is your \"Better them than me\" moment?  r/AskReddit   \n",
       "1566       What are some of the creepiest things you have...  r/AskReddit   \n",
       "\n",
       "                                               interact  \n",
       "1562  people rapy ever thought dating rapist getting...  \n",
       "1563  feeling worthless empty times not sad hollow w...  \n",
       "1564               what genre music mr green commercial  \n",
       "1565                              what better me moment  \n",
       "1566               what creepiest things found internet  "
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "concat_df.tail() # It's clean!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = concat_df['interact']\n",
    "y = concat_df['true_y']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, stratify = y, random_state = 42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "focus": false,
    "id": "db045898-1d2d-4af2-8e79-437c4c7546b4"
   },
   "source": [
    "## NLP\n",
    "\n",
    "#### Use `CountVectorizer` or `TfidfVectorizer` from scikit-learn to create features from the thread titles and descriptions (NOTE: Not all threads have a description)\n",
    "- Examine using count or binary features in the model\n",
    "- Re-evaluate your models using these. Does this improve the model performance? \n",
    "- What text features are the most valuable? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "# vectorize interact feature:\n",
    "vectorizer = CountVectorizer(analyzer = \"word\",   \\\n",
    "                             tokenizer = None,    \\\n",
    "                             preprocessor = None, \\\n",
    "                             stop_words = None,   \\\n",
    "                             lowercase = False,\n",
    "                             max_features = 5000) \n",
    "\n",
    "vectorizer.fit(X_train)\n",
    "\n",
    "X_train_vec = vectorizer.transform(X_train)\n",
    "\n",
    "X_test_vec = vectorizer.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_vec_df = pd.DataFrame(X_train_vec.todense(), \n",
    "                    columns = vectorizer.get_feature_names())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a `RandomForestClassifier` model to predict which subreddit a given post belongs to."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {
    "focus": false,
    "id": "588f9845-6143-4bcc-bfd1-85d45b79303d"
   },
   "outputs": [],
   "source": [
    "## YOUR CODE HERE\n",
    "\n",
    "clf = RandomForestClassifier(n_jobs=2, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
       "            max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
       "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "            min_samples_leaf=1, min_samples_split=2,\n",
       "            min_weight_fraction_leaf=0.0, n_estimators=10, n_jobs=2,\n",
       "            oob_score=False, random_state=0, verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.fit(X_train_vec.todense(), y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9957446808510638"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.score(X_train_vec, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "pred = clf.predict(X_test_vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8903061224489796"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.score(X_test_vec, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.1, 0.9],\n",
       "       [1. , 0. ],\n",
       "       [0.9, 0.1],\n",
       "       [0.1, 0.9],\n",
       "       [0.2, 0.8],\n",
       "       [1. , 0. ],\n",
       "       [1. , 0. ],\n",
       "       [0. , 1. ],\n",
       "       [1. , 0. ],\n",
       "       [1. , 0. ]])"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.predict_proba(X_test_vec)[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1956,  682, 1673, 3564, 1961, 4333, 1917, 3576, 4454, 2919, 3522,\n",
       "       1991, 4060, 2020, 1667, 1670, 4713, 4938,  605, 1437, 1993,  748,\n",
       "        879, 2899, 1777,  800, 2101, 4875,   85, 2002,  891, 2551, 1550,\n",
       "       4046,  618, 1619,  587, 2917, 4447, 4943, 1592,  478, 3852, 2360,\n",
       "       1215, 1803, 3243, 4935, 3677,  914, 4962, 3104, 2999, 4066,  761,\n",
       "        363, 4138, 2014, 4324, 2058, 1521, 1792, 2824, 4662, 1346, 4886,\n",
       "       2161, 3534, 2631, 2431, 2685, 4434, 1238, 4707, 2013, 2225, 1568,\n",
       "        665, 1299, 2365, 4449, 2325, 3160, 1689, 4880,  825, 4951, 1141,\n",
       "       3854, 2066, 4446, 4982, 4042, 4702, 4893, 4436, 3533, 2056, 4872,\n",
       "       4870])"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.argsort(clf.feature_importances_)[4900:5000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_names = pd.DataFrame(X_train_vec.todense(), columns=vectorizer.get_feature_names()).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['gil', 'blue', 'er', 'released', 'give', 'takes', 'friend', 'rememred',\n",
       "       'this', 'north', 'record', 'guys', 'south', 'held', 'english', 'entire',\n",
       "       'using', 'workers', 'baseball', 'died', 'had', 'break', 'cases', 'non',\n",
       "       'favore', 'butterflies', 'inappropriate', 'when', '1980s', 'happen',\n",
       "       'caused', 'male', 'drink', 'song', 'battle', 'egypt', 'band', 'normal',\n",
       "       'things', 'world', 'earth', 'around', 'series', 'larger', 'country',\n",
       "       'female', 'plant', 'work', 'rock', 'century', 'ww2', 'parents', 'one',\n",
       "       'space', 'brish', 'also', 'star', 'head', 'system', 'human', 'done',\n",
       "       'feel', 'named', 'uned', 'deal', 'who', 'instead', 'reddors', 'means',\n",
       "       'life', 'mexico', 'that', 'created', 'used', 'he', 'it', 'due', 'black',\n",
       "       'cy', 'late', 'think', 'known', 'people', 'ever', 'which', 'called',\n",
       "       'would', 'considered', 'serious', 'if', 'thing', 'you', 'something',\n",
       "       'us', 'why', 'the', 'redd', 'how', 'whats', 'what'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Top 100 words\n",
    "\n",
    "feature_names.columns[[1956,  682, 1673, 3564, 1961, 4333, 1917, 3576, 4454, 2919, 3522,\n",
    "       1991, 4060, 2020, 1667, 1670, 4713, 4938,  605, 1437, 1993,  748,\n",
    "        879, 2899, 1777,  800, 2101, 4875,   85, 2002,  891, 2551, 1550,\n",
    "       4046,  618, 1619,  587, 2917, 4447, 4943, 1592,  478, 3852, 2360,\n",
    "       1215, 1803, 3243, 4935, 3677,  914, 4962, 3104, 2999, 4066,  761,\n",
    "        363, 4138, 2014, 4324, 2058, 1521, 1792, 2824, 4662, 1346, 4886,\n",
    "       2161, 3534, 2631, 2431, 2685, 4434, 1238, 4707, 2013, 2225, 1568,\n",
    "        665, 1299, 2365, 4449, 2325, 3160, 1689, 4880,  825, 4951, 1141,\n",
    "       3854, 2066, 4446, 4982, 4042, 4702, 4893, 4436, 3533, 2056, 4872,\n",
    "       4870]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The three most important words were: _'What, whats, and how'._\n",
    "\n",
    "This makes sense because when you ask a question (on r/AskReddit), most questions start with 'what', 'what's', and 'how'. (Why was #4).\n",
    "\n",
    "After deeper exploration, other interesting feature words in top 100 were: 'called', 'people', 'used', 'earth', 'city', 'favorite', 'best', 'black', 'ww2', 'blood', 'created', 'love'."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "focus": false,
    "id": "c7631f51-07f2-4c79-a093-3e9bc7849a48"
   },
   "source": [
    "#### We want to predict a binary variable - class `0` for one of your subreddits and `1` for the other."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {
    "focus": false,
    "id": "c20d2498-151c-44c3-a453-3a333c79a0ac"
   },
   "outputs": [],
   "source": [
    "## YOUR CODE HERE\n",
    "\n",
    "#One hot encoded above."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "focus": false,
    "id": "a7afb2c0-d41e-4779-8216-91cd8dd4473f"
   },
   "source": [
    "#### Thought experiment: What is the baseline accuracy for this model?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "focus": false,
    "id": "87a17d3d-b7f4-4747-9f75-f9af1d18a174"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6024723487312947"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# The baseline accuracy would be the proportion of TIL to AskReddit in the total dataset.\n",
    "# 978 for AskReddit (Class 0)\n",
    "# 631 for TIL (Class 1)\n",
    "\n",
    "# Thus:\n",
    "ask_reddit_baseline = 926/1537\n",
    "ask_reddit_baseline\n",
    "\n",
    "#The model's accuracy scored ~93%. Did a lot better than the baseline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[228,  11],\n",
       "       [ 32, 121]])"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "confusion_matrix(y_test, pred) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "focus": false,
    "id": "9367beff-72ba-4768-a0ba-a50b335de61d"
   },
   "source": [
    "#### Use cross-validation in scikit-learn to evaluate the model above. \n",
    "- Evaluate the accuracy of the model, as well as any other metrics you feel are appropriate. \n",
    "- **Bonus**: Use `GridSearchCV` with `Pipeline` to optimize your `CountVectorizer`/`TfidfVectorizer` and classification model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "focus": false,
    "id": "269b9e7c-60b5-4a06-8255-881d7395bc1b"
   },
   "outputs": [],
   "source": [
    "## YOUR CODE HERE\n",
    "\n",
    "steps = [\n",
    "    ('random_forest', RandomForestClassifier())\n",
    "]\n",
    "\n",
    "pipe = Pipeline(steps)\n",
    "\n",
    "param_grid = {\n",
    "    'random_forest__max_depth' : [30, 50, 70, 80, 100],\n",
    "    'random_forest__n_estimators' : [20, 30, 50, 70, 80, 100]\n",
    "}\n",
    "\n",
    "gs = GridSearchCV(pipe, param_grid, cv = 3)\n",
    "result = gs.fit(X_train_vec, y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'random_forest__max_depth': 100, 'random_forest__n_estimators': 100}"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9276595744680851"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result.best_score_ # Not much difference. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `GradientBoost` with GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'learning_rate': 0.1, 'max_depth': 2, 'n_estimators': 80}\n",
      "0.9347079037800687\n"
     ]
    }
   ],
   "source": [
    "## Gradient Boost\n",
    "\n",
    "gradient_boost = GradientBoostingClassifier()\n",
    "\n",
    "grad_params = {\n",
    "    'n_estimators' : [80, 90, 100],\n",
    "    'learning_rate' : [0.09, 0.1, 0.2, 0.5],\n",
    "    'max_depth' : [1, 2, 4, 6]\n",
    "}\n",
    "\n",
    "g_search = GridSearchCV(gradient_boost, param_grid = grad_params, cv = 3)\n",
    "g_search.fit(X_train_vec, y_train)\n",
    "print(g_search.best_params_)\n",
    "print(g_search.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9278350515463918"
      ]
     },
     "execution_count": 190,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "g_search.score(X_test_vec, y_test) #model is slightly overfitting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `Decision Tree` with GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9344563726532714"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Decision Tree\n",
    "\n",
    "dt = DecisionTreeClassifier(max_depth= 20)\n",
    "cross_val_score(dt, X_train_vec, y_train, cv = 5).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9744680851063829"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dt.fit(X_train_vec, y_train)\n",
    "dt.score(X_train_vec, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9311224489795918"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dt.score(X_test_vec, y_test) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=5, error_score='raise',\n",
       "       estimator=DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=None,\n",
       "            max_features=None, max_leaf_nodes=None,\n",
       "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "            min_samples_leaf=1, min_samples_split=2,\n",
       "            min_weight_fraction_leaf=0.0, presort=False, random_state=None,\n",
       "            splitter='best'),\n",
       "       fit_params=None, iid=True, n_jobs=1,\n",
       "       param_grid={'max_depth': [10, 20, 30, 50, 100, 200]},\n",
       "       pre_dispatch='2*n_jobs', refit=True, return_train_score='warn',\n",
       "       scoring=None, verbose=0)"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dt = DecisionTreeClassifier()\n",
    "\n",
    "dt_params = {\n",
    "    'max_depth' : [10, 20, 30, 50, 100, 200],\n",
    "}\n",
    "\n",
    "gridsearch = GridSearchCV(dt, param_grid = dt_params, cv = 5)\n",
    "gridsearch.fit(X_train_vec, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'max_depth': 20}"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gridsearch.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9226804123711341"
      ]
     },
     "execution_count": 198,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gridsearch.score(X_test_vec, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Executive Summary\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Random Forest had a 99% training accuracy score and a 93% test score.\n",
    "2. Gradient boost had a 93% training accuracy score and a 92% test score.\n",
    "3. Decision Tree had a 97% training accuracy score and a 92% test score.\n",
    "\n",
    "Using feature_importance_, I found that the most important word was 'What'. This makes sense because most of the posts that belonged to 'AskReddit' starts with 'What'. \n",
    "\n",
    "In r/todayilearned, all of the posts begin with 'TIL'. I took that keyword out because this would make the classifying too easy. Although this keyword was taken out, having a 92% accuracy rate across three different models is promising! \n",
    "\n",
    "Observing which key words were most important provides insight into what kind of questions most people asked. Seems like a lot of people had scientific questions relating to their physical surroundings judging from keywords such as, 'city', 'earth', and 'called'. I also think people were interested in societal questions judging from keywords such as, 'black' and 'love'. Historical questions on AskReddit were popular too, 'WW2' came up once.\n",
    "\n",
    "In the future, I would like to experiment taking out 'What' and 'Whats' and see the change in accuracy rate. Since we also know what types of questions are popular, I would like to know if there's a correlation on Reddit Gold and asking  or answering scientific/historical/societal questions.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
